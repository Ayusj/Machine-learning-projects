{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8252a5-ef67-43e3-94d9-7b59c900497c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf4a7a-9fe3-4195-8f05-8458a0e945b5",
   "metadata": {},
   "source": [
    "You're absolutely right, and I apologize for the confusion earlier. Let's walk through the correct gradient descent formulas and final understanding based on your explanation.\n",
    "\n",
    "Cost Function (J)\n",
    "The cost function combines the reconstruction error (mean squared error) and regularization to minimize the difference between the predicted and actual ratings. Here's the updated version, considering your specifications:\n",
    "\n",
    "𝐽\n",
    "(\n",
    "𝑋\n",
    ",\n",
    "𝑊\n",
    ",\n",
    "𝑏\n",
    ")\n",
    "=\n",
    "1\n",
    "2\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑚\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "𝑀\n",
    "𝑖\n",
    "𝑗\n",
    "(\n",
    "(\n",
    "𝑊\n",
    "𝑗\n",
    "⋅\n",
    "𝑋\n",
    "𝑖\n",
    "+\n",
    "𝑏\n",
    "𝑗\n",
    "−\n",
    "𝑌\n",
    "𝑖\n",
    "𝑗\n",
    ")\n",
    "2\n",
    ")\n",
    "+\n",
    "𝜆\n",
    "2\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑚\n",
    "∑\n",
    "𝑘\n",
    "=\n",
    "1\n",
    "𝐾\n",
    "𝑋\n",
    "𝑖\n",
    "𝑘\n",
    "2\n",
    "+\n",
    "𝜆\n",
    "2\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "𝑘\n",
    "=\n",
    "1\n",
    "𝐾\n",
    "𝑊\n",
    "𝑗\n",
    "𝑘\n",
    "2\n",
    "+\n",
    "𝜆\n",
    "2\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "𝑏\n",
    "𝑗\n",
    "2\n",
    "J(X,W,b)= \n",
    "2\n",
    "1\n",
    "​\n",
    "  \n",
    "i=1\n",
    "∑\n",
    "m\n",
    "​\n",
    "  \n",
    "j=1\n",
    "∑\n",
    "n\n",
    "​\n",
    " M \n",
    "ij\n",
    "​\n",
    " ((W \n",
    "j\n",
    "​\n",
    " ⋅X \n",
    "i\n",
    "​\n",
    " +b \n",
    "j\n",
    "​\n",
    " −Y \n",
    "ij\n",
    "​\n",
    " ) \n",
    "2\n",
    " )+ \n",
    "2\n",
    "λ\n",
    "​\n",
    "  \n",
    "i=1\n",
    "∑\n",
    "m\n",
    "​\n",
    "  \n",
    "k=1\n",
    "∑\n",
    "K\n",
    "​\n",
    " X \n",
    "ik\n",
    "2\n",
    "​\n",
    " + \n",
    "2\n",
    "λ\n",
    "​\n",
    "  \n",
    "j=1\n",
    "∑\n",
    "n\n",
    "​\n",
    "  \n",
    "k=1\n",
    "∑\n",
    "K\n",
    "​\n",
    " W \n",
    "jk\n",
    "2\n",
    "​\n",
    " + \n",
    "2\n",
    "λ\n",
    "​\n",
    "  \n",
    "j=1\n",
    "∑\n",
    "n\n",
    "​\n",
    " b \n",
    "j\n",
    "2\n",
    "​\n",
    " \n",
    "Where:\n",
    "\n",
    "𝑌\n",
    "𝑖\n",
    "𝑗\n",
    "Y \n",
    "ij\n",
    "​\n",
    "  is the rating given by user \n",
    "𝑗\n",
    "j to movie \n",
    "𝑖\n",
    "i,\n",
    "𝑀\n",
    "𝑖\n",
    "𝑗\n",
    "M \n",
    "ij\n",
    "​\n",
    "  is a binary mask indicating whether a rating exists for that user and movie pair (1 if rated, 0 if not),\n",
    "𝑊\n",
    "𝑗\n",
    "W \n",
    "j\n",
    "​\n",
    "  is the feature vector for the \n",
    "𝑗\n",
    "j-th user,\n",
    "𝑋\n",
    "𝑖\n",
    "X \n",
    "i\n",
    "​\n",
    "  is the feature vector for the \n",
    "𝑖\n",
    "i-th movie,\n",
    "𝑏\n",
    "𝑗\n",
    "b \n",
    "j\n",
    "​\n",
    "  is the bias term for user \n",
    "𝑗\n",
    "j,\n",
    "𝜆\n",
    "λ is the regularization parameter.\n",
    "Gradient Descent Update Rules\n",
    "Now, based on your updated logic, the gradient descent update formulas are as follows:\n",
    "\n",
    "For \n",
    "𝑋\n",
    "[\n",
    "𝑖\n",
    ",\n",
    "𝑘\n",
    "]\n",
    "X[i,k] (movie features):\n",
    "𝑋\n",
    "[\n",
    "𝑖\n",
    ",\n",
    "𝑘\n",
    "]\n",
    ":\n",
    "=\n",
    "𝑋\n",
    "[\n",
    "𝑖\n",
    ",\n",
    "𝑘\n",
    "]\n",
    "−\n",
    "𝛼\n",
    "⋅\n",
    "∂\n",
    "𝐽\n",
    "∂\n",
    "𝑋\n",
    "[\n",
    "𝑖\n",
    ",\n",
    "𝑘\n",
    "]\n",
    "X[i,k]:=X[i,k]−α⋅ \n",
    "∂X[i,k]\n",
    "∂J\n",
    "​\n",
    " \n",
    "The gradient \n",
    "∂\n",
    "𝐽\n",
    "∂\n",
    "𝑋\n",
    "[\n",
    "𝑖\n",
    ",\n",
    "𝑘\n",
    "]\n",
    "∂X[i,k]\n",
    "∂J\n",
    "​\n",
    "  is:\n",
    "\n",
    "∂\n",
    "𝐽\n",
    "∂\n",
    "𝑋\n",
    "[\n",
    "𝑖\n",
    ",\n",
    "𝑘\n",
    "]\n",
    "=\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "𝑀\n",
    "[\n",
    "𝑖\n",
    ",\n",
    "𝑗\n",
    "]\n",
    "(\n",
    "(\n",
    "𝑊\n",
    "𝑗\n",
    "⋅\n",
    "𝑋\n",
    "𝑖\n",
    "+\n",
    "𝑏\n",
    "𝑗\n",
    "−\n",
    "𝑌\n",
    "𝑖\n",
    "𝑗\n",
    ")\n",
    ")\n",
    "⋅\n",
    "𝑊\n",
    "𝑗\n",
    "𝑘\n",
    "+\n",
    "𝜆\n",
    "𝑋\n",
    "𝑖\n",
    "𝑘\n",
    "∂X[i,k]\n",
    "∂J\n",
    "​\n",
    " = \n",
    "j=1\n",
    "∑\n",
    "n\n",
    "​\n",
    " M[i,j]((W \n",
    "j\n",
    "​\n",
    " ⋅X \n",
    "i\n",
    "​\n",
    " +b \n",
    "j\n",
    "​\n",
    " −Y \n",
    "ij\n",
    "​\n",
    " ))⋅W \n",
    "jk\n",
    "​\n",
    " +λX \n",
    "ik\n",
    "​\n",
    " \n",
    "For \n",
    "𝑊\n",
    "[\n",
    "𝑗\n",
    ",\n",
    "𝑘\n",
    "]\n",
    "W[j,k] (user features):\n",
    "𝑊\n",
    "[\n",
    "𝑗\n",
    ",\n",
    "𝑘\n",
    "]\n",
    ":\n",
    "=\n",
    "𝑊\n",
    "[\n",
    "𝑗\n",
    ",\n",
    "𝑘\n",
    "]\n",
    "−\n",
    "𝛼\n",
    "⋅\n",
    "∂\n",
    "𝐽\n",
    "∂\n",
    "𝑊\n",
    "[\n",
    "𝑗\n",
    ",\n",
    "𝑘\n",
    "]\n",
    "W[j,k]:=W[j,k]−α⋅ \n",
    "∂W[j,k]\n",
    "∂J\n",
    "​\n",
    " \n",
    "The gradient \n",
    "∂\n",
    "𝐽\n",
    "∂\n",
    "𝑊\n",
    "[\n",
    "𝑗\n",
    ",\n",
    "𝑘\n",
    "]\n",
    "∂W[j,k]\n",
    "∂J\n",
    "​\n",
    "  is:\n",
    "\n",
    "∂\n",
    "𝐽\n",
    "∂\n",
    "𝑊\n",
    "[\n",
    "𝑗\n",
    ",\n",
    "𝑘\n",
    "]\n",
    "=\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑚\n",
    "𝑀\n",
    "[\n",
    "𝑖\n",
    ",\n",
    "𝑗\n",
    "]\n",
    "(\n",
    "(\n",
    "𝑊\n",
    "𝑗\n",
    "⋅\n",
    "𝑋\n",
    "𝑖\n",
    "+\n",
    "𝑏\n",
    "𝑗\n",
    "−\n",
    "𝑌\n",
    "𝑖\n",
    "𝑗\n",
    ")\n",
    ")\n",
    "⋅\n",
    "𝑋\n",
    "𝑖\n",
    "𝑘\n",
    "+\n",
    "𝜆\n",
    "𝑊\n",
    "𝑗\n",
    "𝑘\n",
    "∂W[j,k]\n",
    "∂J\n",
    "​\n",
    " = \n",
    "i=1\n",
    "∑\n",
    "m\n",
    "​\n",
    " M[i,j]((W \n",
    "j\n",
    "​\n",
    " ⋅X \n",
    "i\n",
    "​\n",
    " +b \n",
    "j\n",
    "​\n",
    " −Y \n",
    "ij\n",
    "​\n",
    " ))⋅X \n",
    "ik\n",
    "​\n",
    " +λW \n",
    "jk\n",
    "​\n",
    " \n",
    "For \n",
    "𝑏\n",
    "[\n",
    "𝑗\n",
    "]\n",
    "b[j] (user biases):\n",
    "𝑏\n",
    "[\n",
    "𝑗\n",
    "]\n",
    ":\n",
    "=\n",
    "𝑏\n",
    "[\n",
    "𝑗\n",
    "]\n",
    "−\n",
    "𝛼\n",
    "⋅\n",
    "∂\n",
    "𝐽\n",
    "∂\n",
    "𝑏\n",
    "[\n",
    "𝑗\n",
    "]\n",
    "b[j]:=b[j]−α⋅ \n",
    "∂b[j]\n",
    "∂J\n",
    "​\n",
    " \n",
    "The gradient \n",
    "∂\n",
    "𝐽\n",
    "∂\n",
    "𝑏\n",
    "[\n",
    "𝑗\n",
    "]\n",
    "∂b[j]\n",
    "∂J\n",
    "​\n",
    "  is:\n",
    "\n",
    "∂\n",
    "𝐽\n",
    "∂\n",
    "𝑏\n",
    "[\n",
    "𝑗\n",
    "]\n",
    "=\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑚\n",
    "𝑀\n",
    "[\n",
    "𝑖\n",
    ",\n",
    "𝑗\n",
    "]\n",
    "(\n",
    "(\n",
    "𝑊\n",
    "𝑗\n",
    "⋅\n",
    "𝑋\n",
    "𝑖\n",
    "+\n",
    "𝑏\n",
    "𝑗\n",
    "−\n",
    "𝑌\n",
    "𝑖\n",
    "𝑗\n",
    ")\n",
    ")\n",
    "∂b[j]\n",
    "∂J\n",
    "​\n",
    " = \n",
    "i=1\n",
    "∑\n",
    "m\n",
    "​\n",
    " M[i,j]((W \n",
    "j\n",
    "​\n",
    " ⋅X \n",
    "i\n",
    "​\n",
    " +b \n",
    "j\n",
    "​\n",
    " −Y \n",
    "ij\n",
    "​\n",
    " ))\n",
    "Algorithm Overview\n",
    "Initialize: Initialize feature matrices \n",
    "𝑊\n",
    "W, \n",
    "𝑋\n",
    "X, and bias vector \n",
    "𝑏\n",
    "b randomly (or with small random values).\n",
    "For each iteration:\n",
    "Compute the predicted ratings using the current feature matrices and biases.\n",
    "Calculate the cost function with regularization.\n",
    "Compute the gradients for \n",
    "𝑋\n",
    "X, \n",
    "𝑊\n",
    "W, and \n",
    "𝑏\n",
    "b using the formulas above.\n",
    "Update the parameters using the gradients and learning rate \n",
    "𝛼\n",
    "α.\n",
    "Convergence: Stop when the cost function reaches a minimum or after a predefined number of iterations.\n",
    "Prediction:\n",
    "The prediction for the rating of movie \n",
    "𝑖\n",
    "i by user \n",
    "𝑗\n",
    "j is computed as:\n",
    "\n",
    "𝑌\n",
    "𝑖\n",
    "𝑗\n",
    "=\n",
    "𝑊\n",
    "𝑗\n",
    "⋅\n",
    "𝑋\n",
    "𝑖\n",
    "+\n",
    "𝑏\n",
    "𝑗\n",
    "Y \n",
    "ij\n",
    "​\n",
    " =W \n",
    "j\n",
    "​\n",
    " ⋅X \n",
    "i\n",
    "​\n",
    " +b \n",
    "j\n",
    "​\n",
    " \n",
    "Regularization:\n",
    "The regularization term helps prevent overfitting by adding penalties for large values of the weights, feature vectors, and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac172e-5118-4070-985b-a63851a6e9fb",
   "metadata": {},
   "source": [
    "### implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "999f2aa3-dc92-46e2-a3eb-6373c8384a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import copy, math\n",
    "# scaler = StandardScaler()\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from recsys_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e134cd72-6139-4fff-9810-f4a452bafc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['User ID', 'Movie ID', 'Rating', 'Timestamp']\n",
    "df = pd.read_csv('../../notes and data/ml-100k/u.data',sep='\\t', names=column_names)\n",
    "df = df.drop(columns=['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662e4ea3-f93c-4299-8af8-70c2661d3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['Rating']] = scaler.fit_transform(df[['Rating']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33eb8e09-f5b1-4fc6-b5f7-a46291f0d2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 4., 0., ..., 5., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 5.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.pivot(index='Movie ID', columns='User ID', values='Rating')\n",
    "y = y.fillna(0)\n",
    "y = np.array(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "790510b1-7342-4daa-b691-e50254267c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1682, 943) (1682, 943)\n"
     ]
    }
   ],
   "source": [
    "M = (y != 0).astype(int)\n",
    "print(y.shape,M.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13f20b84-9345-4e4e-a4ca-8304be7eeb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def costFn(x,y,w,b,lam):\n",
    "#     cost = 0\n",
    "#     pred = 0\n",
    "#     reg = 0\n",
    "#     for i in range(y.shape[0]):\n",
    "#         for j in range(y.shape[1]):\n",
    "#             if m[i][j] == 1:\n",
    "#                 val = np.dot(w[j],x[i]) + b[j] - y[i][j]\n",
    "#                 pred += (val**2)/2\n",
    "                \n",
    "#     for i in range(y.shape[0]):  # Regularization for X\n",
    "#         reg += np.sum(x[i] ** 2)\n",
    "\n",
    "#     for j in range(y.shape[1]):  # Regularization for W and b\n",
    "#         reg += np.sum(w[j] ** 2) + b[j] ** 2\n",
    "\n",
    "#     reg = (lam/2)*reg\n",
    "\n",
    "#     cost = pred + reg  # Divide by 2m for proper scaling\n",
    "#     return cost\n",
    "\n",
    "\n",
    "def costFn(X, Y,W, b,m, lam):\n",
    "    \n",
    "    # nm, nu = Y.shape\n",
    "    # J = 0\n",
    "    # ### START CODE HERE ###  \n",
    "    # for j in range(nu):\n",
    "    #     w = W[j,:]\n",
    "    #     b_j = b[0,j]\n",
    "    #     for i in range(nm):\n",
    "    #         x = X[i,:]\n",
    "    #         y = Y[i,j]\n",
    "    #         r = m[i,j]\n",
    "    #         J += r * np.square((np.dot(w,x) + b_j - y ))\n",
    "    # J += (lam) * (np.sum(np.square(W)) + np.sum(np.square(X)))\n",
    "    # J = J/2\n",
    "    # ### END CODE HERE ### \n",
    "\n",
    "    # return J\n",
    "    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*m\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lam/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26380c60-26af-46bd-ab2b-48edd3f1e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientfn(x,y,w,b,m,lam):\n",
    "    djx = np.zeros_like(x)\n",
    "    djw = np.zeros_like(w)\n",
    "    djb = np.zeros_like(b)\n",
    "    for i in range(y.shape[0]):\n",
    "        for k in range(len(x[i])):\n",
    "            for j in range(y.shape[1]):\n",
    "                if m[i][j] == 1:\n",
    "                    val = np.dot(w[j],x[i]) + b[0,j] - y[i][j]\n",
    "                    djx[i][k] += val*w[j][k]\n",
    "            djx[i][k] += lam*x[i][k]\n",
    "            \n",
    "    for j in range(y.shape[1]):\n",
    "        for k in range(len(w[j])):\n",
    "            for i in range(y.shape[0]):\n",
    "                if m[i][j] == 1:\n",
    "                    val = np.dot(w[j],x[i]) + b[0,j] - y[i][j]\n",
    "                    djw[j][k] += val*x[i][k]\n",
    "            djw[j][k] += lam*w[j][k]\n",
    "            \n",
    "    for j in range(y.shape[1]):\n",
    "        for i in range(y.shape[0]):\n",
    "            if m[i][j] == 1:\n",
    "                val = np.dot(w[j],x[i]) + b[0,j] - y[i][j]\n",
    "                djb[0,j] += val    \n",
    "    \n",
    "    return djw, djb ,djx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df29c866-30c2-4b78-b003-8c1beb0777f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDecent(x,y,w_in,b_in,alpha,iters,lam,m):\n",
    "    # x = copy.deepcopy(x)\n",
    "    # w = copy.deepcopy(w_in) #avoid modifying global w within function\n",
    "    # b = copy.deepcopy(b_in)\n",
    "    # for i in range(iters):\n",
    "    #     djw,djb,djx = gradientfn(x,y,w,b,m,lam)\n",
    "    #     w = w - alpha*djw\n",
    "    #     b = b - alpha*djb\n",
    "    #     x = x - alpha*djx\n",
    "    #     # if i % 100 == 0 and i > 0:\n",
    "    #     #     alpha *= 0.9\n",
    "    #     if i % math.ceil(iters / 10) == 0:\n",
    "    #         cost = costFn(x,y,w,b,m,lam)\n",
    "    #         print(f\"Iteration {i:4d}: Cost {cost:.4f}\")\n",
    "    for iter in range(iters):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute the cost (forward pass included in costFn)\n",
    "            cost_value = costFn(x, y, w_in, b_in, m, lam)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(cost_value, [x, w_in, b_in])\n",
    "        \n",
    "        # Apply gradients to variables\n",
    "        optimizer.apply_gradients(zip(grads, [x, w_in, b_in]))\n",
    "\n",
    "        # Log periodically\n",
    "        if iter % 20 == 0:\n",
    "            print(f\"Training loss at iteration {iter}: {cost_value.numpy():0.1f}\")\n",
    "    \n",
    "    return w, b, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ba07f8f-1b48-4a67-a71c-e57037e0ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(xt, wt, bt):\n",
    "    print(xt.shape,wt.shape,bt.shape)\n",
    "    # predicted_ratings = np.dot(x, w.T) + b  # Matrix multiplication for all users and all movies\n",
    "    predicted_ratings = np.matmul(xt.numpy(), np.transpose(wt.numpy())) + bt.numpy()\n",
    "    return predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db5cc11d-e830-4af6-b808-817ce6980381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682 754\n"
     ]
    }
   ],
   "source": [
    "num_features = 100\n",
    "num_movies, num_users = y.shape  # Total number of users (943)\n",
    "subset_users = num_users # Number of users you want to use for faster processing\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.5)\n",
    "# Randomly select a subset of users\n",
    "user_indices = np.arange(num_users)\n",
    "selected_users = np.random.choice(user_indices, size=subset_users, replace=False)\n",
    "\n",
    "# Split the selected users into training and test sets (80% train, 20% test)\n",
    "train_users, test_users = train_test_split(selected_users, test_size=0.2, random_state=42)\n",
    "\n",
    "# Select the corresponding data for the training and test sets\n",
    "y_train = y[:, train_users]  \n",
    "y_test = y[:, test_users]    \n",
    "\n",
    "M_train = M[:, train_users]  \n",
    "M_test = M[:, test_users]    \n",
    "\n",
    "nm,nu = y_train.shape\n",
    "print(nm,nu)\n",
    "\n",
    "alpha = 0.01  # Reduce the learning rate to avoid divergence\n",
    "iterations = 200\n",
    "lam = 1\n",
    "\n",
    "# Initialize features and weights with small random values instead of zeros\n",
    "# initial_x = np.random.randn(y_train.shape[0], 10) * 0.1  # Small random values\n",
    "# initial_w = np.random.randn(y_train.shape[1], 10) * 0 # Small random values\n",
    "# initial_b = np.random.randn(y_train.shape[1]) * 0  # Small random values\n",
    "# print(initial_x.shape,initial_w.shape,initial_b.shape)\n",
    "\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "\n",
    "w = tf.Variable(tf.random.normal((nu,  num_features),dtype=tf.float64),  name='w')\n",
    "x = tf.Variable(tf.random.normal((nm, num_features),dtype=tf.float64),  name='x')\n",
    "b = tf.Variable(tf.random.normal((1,          nu),   dtype=tf.float64),  name='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4feb6766-af07-4c55-87d7-2f79d725ba02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at iteration 0: 4730887.9\n",
      "Training loss at iteration 20: 151020.2\n",
      "Training loss at iteration 40: 36455.0\n",
      "Training loss at iteration 60: 11851.4\n",
      "Training loss at iteration 80: 6775.2\n",
      "Training loss at iteration 100: 5503.0\n",
      "Training loss at iteration 120: 5075.1\n",
      "Training loss at iteration 140: 4883.2\n",
      "Training loss at iteration 160: 4776.8\n",
      "Training loss at iteration 180: 4709.4\n"
     ]
    }
   ],
   "source": [
    "w,b,x = gradientDecent(x,y_train,w,b,alpha,iterations,lam,M_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf70a684-a92f-4a0e-b3f8-f9250e7a7d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1682, 100) (754, 100) (1, 754)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,w.shape,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca59bcd9-d341-4e2e-a720-4dfd25701a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1682, 100) (754, 100) (1, 754)\n",
      "[4. 5. 4. ... 4. 3. 3.]\n",
      "[4.11597664 5.05895166 4.06278109 ... 3.98567781 2.99868701 3.01018467]\n",
      "0.9939252525157379\n",
      "0.9992213084937138\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(x, w, b)\n",
    "y_pred_masked = y_pred * M_train\n",
    "y_masked = y_train * M_train\n",
    "y_flat = y_masked[M_train > 0]\n",
    "y_pred_flat = y_pred_masked[M_train > 0]\n",
    "mse = mean_squared_error(y_flat, y_pred_flat)\n",
    "rmse = np.sqrt(mse)\n",
    "print(y_flat)\n",
    "print(y_pred_flat)\n",
    "print(r2_score(y_flat, y_pred_flat))\n",
    "print(r2_score(y_masked, y_pred_masked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "657c265e-0f3f-46a0-b92a-64635686baca",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1329660725.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[37], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    [4. 5. 4. ... 4. 3. 4.]\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "(1682, 100) (40, 100) (1, 40)\n",
    "[4. 5. 4. ... 4. 3. 4.]\n",
    "[3.99191436 4.80398109 3.99441594 ... 3.97959176 3.02567236 3.95175007]\n",
    "0.9931301625618753\n",
    "0.9988648010094924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec5c6596-d1ea-4816-9dd8-7aa73a889ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1682, 100) (754, 100) (1, 754)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1682, 754)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = predict(x, w, b)\n",
    "y_pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f176f621-29de-47ac-8c71-78e60458f6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Optimization Iteration 0: Loss 205156.6626\n",
      "Test Optimization Iteration 20: Loss 26422.6467\n",
      "Test Optimization Iteration 40: Loss 10388.3476\n",
      "Test Optimization Iteration 60: Loss 6210.2161\n",
      "Test Optimization Iteration 80: Loss 4777.4963\n",
      "Test Optimization Iteration 100: Loss 4173.6386\n",
      "Test Optimization Iteration 120: Loss 3888.3166\n",
      "Test Optimization Iteration 140: Loss 3742.2867\n",
      "Test Optimization Iteration 160: Loss 3662.6163\n",
      "Test Optimization Iteration 180: Loss 3617.1098\n",
      "0.9857757554600043\n",
      "Test RMSE: 0.5493\n",
      "Test R^2 Score: 0.7468\n"
     ]
    }
   ],
   "source": [
    "# Fix learned movie features from training\n",
    "X_fixed = x.numpy()  \n",
    "\n",
    "# Initialize test users' parameters\n",
    "w_test = tf.Variable(tf.random.normal((len(test_users), num_features), dtype=tf.float64), name='w_test')\n",
    "b_test = tf.Variable(tf.random.normal((1, len(test_users)), dtype=tf.float64), name='b_test')\n",
    "lam_test=10\n",
    "# Optimize for test users\n",
    "def test_loss_fn(X, W_test, B_test, Y_test, M_test, lam_test):\n",
    "    residual = (tf.linalg.matmul(X, tf.transpose(W_test)) + B_test - Y_test) * M_test\n",
    "    loss = 0.5 * tf.reduce_sum(residual ** 2) + (lam / 2) * tf.reduce_sum(W_test ** 2)\n",
    "    return loss\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.1)\n",
    "for iter in range(200):  # Optimize for test users\n",
    "    with tf.GradientTape() as tape:\n",
    "        test_loss = test_loss_fn(X_fixed, w_test, b_test, y_test, M_test, lam)\n",
    "    grads = tape.gradient(test_loss, [w_test, b_test])\n",
    "    optimizer.apply_gradients(zip(grads, [w_test, b_test]))\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"Test Optimization Iteration {iter}: Loss {test_loss.numpy():.4f}\")\n",
    "\n",
    "# Predict ratings for test set after optimization\n",
    "y_test_pred = np.matmul(X_fixed, np.transpose(w_test.numpy())) + b_test.numpy()\n",
    "\n",
    "# Apply mask and evaluate\n",
    "y_test_pred_masked = y_test_pred * M_test\n",
    "y_test_flat = y_test[M_test > 0]\n",
    "y_test_pred_flat = y_test_pred_masked[M_test > 0]\n",
    "\n",
    "mse_test = mean_squared_error(y_test_flat, y_test_pred_flat)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test_flat, y_test_pred_flat)\n",
    "print(r2_score(y_test, y_test_pred_masked))\n",
    "print(f\"Test RMSE: {rmse_test:.4f}\")\n",
    "print(f\"Test R^2 Score: {r2_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5b7be-44a8-4db0-8aa5-88fae7abf735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8933e8-25b0-4a45-b88e-f98b41dc3aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8482f-0e55-472e-84bf-6440b561ed44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
